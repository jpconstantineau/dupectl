# Implementation Plan: Duplicate Scan System

**Branch**: `001-duplicate-scan-system` | **Date**: December 23, 2025 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/001-duplicate-scan-system/spec.md`

**Note**: This plan is generated by the `/speckit.plan` command following the Phase 0 and Phase 1 workflow.

## Summary

Implement a duplicate file and folder detection system for the DupeCTL CLI. The system provides three scan modes: (1) scan all - complete folder traversal with file hashing, (2) scan folders - structure mapping only, (3) scan files - hash calculation for previously registered folders. Files are identified as duplicates when both size and cryptographic hash match. Folders are duplicates when all files in their subtrees match. The system supports partial folder duplicate detection with configurable similarity thresholds (50% minimum). Scanning operations resume from checkpoints after interruption, provide progress feedback every 10 seconds (configurable), and handle permission errors gracefully by marking files in database. Results are queryable in table or JSON format with optional filtering.

## Technical Context

**Language/Version**: Go 1.21+ (as specified in go.mod)  
**Primary Dependencies**: 
  - `spf13/cobra` - CLI framework (existing)
  - `spf13/viper` - Configuration management (existing)
  - `modernc.org/sqlite` - Database (existing)
  - Go stdlib `crypto/sha256`, `crypto/sha512`, `crypto/sha3` - Hash algorithms
  - Go stdlib `path/filepath` - Cross-platform path handling
  - Go stdlib `os`, `io` - File system operations
  
**Storage**: SQLite database (dupedb.db) with WAL mode enabled (existing)  
**Testing**: Go standard testing package with table-driven tests, coverage target ≥70%  
**Target Platform**: Cross-platform CLI (Windows, Linux, macOS)  
**Project Type**: Single project - Go CLI application with library packages  
**Performance Goals**: 
  - File hashing: ≥50 MB/sec (spec requirement)
  - Folder scan: ≥1000 files/sec metadata collection
  - Memory: <500 MB for 100k files
  
**Constraints**: 
  - Progress updates every 10 seconds (configurable)
  - Checkpoint-based scan resumption (folder-level granularity)
  - Cross-platform path handling required
  - Permission errors must not crash process
  
**Scale/Scope**: 
  - Support 100k+ files in single scan
  - Handle deep folder hierarchies (1000+ levels)
  - Multiple root folder registrations

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

Verify compliance with DupeCTL Constitution (v1.0.0):

- [x] **Clean Code & Quality**: Functions single-purpose, descriptive names, <50 lines, low complexity - Scanner, hasher, detector modules will follow clean code principles
- [x] **Clean Architecture**: Domain logic separated, dependencies point inward, no circular deps - Scanner logic in pkg/scanner, database in pkg/datastore, CLI in cmd/
- [x] **Testing**: Test coverage ≥70%, test types identified (unit/integration/contract/e2e) - Unit tests for hash calculation, integration tests for database, e2e for CLI commands
- [x] **UX Consistency**: Commands follow verb-noun pattern, error messages actionable, progress indication - `scan all <path>`, `get duplicates`, progress every 10s
- [x] **Performance**: Meets defined thresholds (hash speed, scan rate, API latency, memory limits) - Spec defines 50MB/sec hash, <500MB memory for 100k files
- [x] **Dependencies**: Uses Go stdlib first, new dependencies justified and approved - Using stdlib crypto for hashing, existing cobra/viper/sqlite
- [x] **Maintainability**: GoDoc comments, no hardcoded paths, config via env/flags - Hash algorithm configurable, progress interval configurable
- [x] **Upgradability**: Database migrations planned, backward compatibility maintained - New tables (files, folders, scan_state) will have migration scripts
- [x] **Observability**: Logging strategy defined, progress visibility, metrics identified - Console progress every 10s, permission errors to console, scan summary stats
- [x] **Portability**: Cross-platform path handling, no OS-specific assumptions - filepath.Join, absolute path normalization, platform-agnostic separators
- [x] **Graceful Shutdown**: Signal handling planned, state persistence designed, resume capability - Checkpoint at folder boundaries, scan_state table tracks progress
- [x] **12-Factor CLI**: Config external, stateless commands, disposable processes, structured logs - Viper config, no persistent state in process, resume via database

**Violations Requiring Justification**: None - all constitution principles can be met within this feature scope.

## Project Structure

### Documentation (this feature)

```text
specs/001-duplicate-scan-system/
├── plan.md              # This file (implementation plan)
├── research.md          # Phase 0 output (hash algorithm analysis, performance patterns)
├── data-model.md        # Phase 1 output (database schema, entity relationships)
├── quickstart.md        # Phase 1 output (user guide for scan commands)
├── contracts/           # Phase 1 output (CLI command specifications)
│   ├── scan-commands.md # scan all/folders/files command contracts
│   └── query-commands.md # get duplicates command contracts
└── tasks.md             # Phase 2 output (/speckit.tasks command - NOT created here)
```

### Source Code (repository root)

```text
dupectl/
├── cmd/                 # CLI commands (thin adapters)
│   ├── scan.go         # Parent scan command
│   ├── scanAll.go      # Implement scan all <path>
│   ├── scanFolders.go  # Implement scan folders <path>
│   ├── scanFiles.go    # Implement scan files <path>
│   ├── getDuplicates.go # Implement get duplicates with --json, --min-count
│   ├── addRoot.go      # Enhance with path validation, confirmation prompt
│   └── ... (other existing commands)
│
├── pkg/
│   ├── scanner/        # NEW: Core scanning logic
│   │   ├── scanner.go  # Scanner interface and factory
│   │   ├── folder.go   # Folder traversal implementation
│   │   ├── file.go     # File processing implementation
│   │   ├── progress.go # Progress tracking and reporting
│   │   └── checkpoint.go # Scan state persistence and resumption
│   │
│   ├── hash/           # NEW: File hashing
│   │   ├── hasher.go   # Hasher interface and factory
│   │   ├── sha256.go   # SHA-256 implementation
│   │   ├── sha512.go   # SHA-512 implementation
│   │   └── sha3.go     # SHA3-256 implementation
│   │
│   ├── detector/       # NEW: Duplicate detection logic
│   │   ├── files.go    # File duplicate matching (size + hash)
│   │   ├── folders.go  # Exact folder duplicate detection
│   │   └── partial.go  # Partial folder duplicate detection
│   │
│   ├── entities/       # EXISTING: Domain models (enhance)
│   │   ├── files.go    # Add File, Folder, ScanState entities
│   │   └── statuses.go # (existing)
│   │
│   ├── datastore/      # EXISTING: Database access (enhance)
│   │   ├── datastore.go    # (existing)
│   │   ├── schema.go       # NEW: Schema definitions and migrations
│   │   ├── files.go        # NEW: File table operations
│   │   ├── folders.go      # NEW: Folder table operations
│   │   ├── scan_state.go   # NEW: Checkpoint operations
│   │   ├── duplicates.go   # NEW: Duplicate query operations
│   │   └── agent.go        # (existing)
│   │
│   ├── formatter/      # NEW: Output formatting
│   │   ├── table.go    # Human-readable table output
│   │   └── json.go     # JSON output
│   │
│   └── ... (other existing packages)
│
└── tests/
    ├── unit/           # Unit tests for hash, scanner, detector
    ├── integration/    # Database and filesystem integration tests
    └── e2e/            # End-to-end CLI command tests
```

**Structure Decision**: Single project layout following existing DupeCTL structure. New scanner, hash, detector, and formatter packages added to pkg/ for scanning capabilities. CLI commands enhanced in cmd/. Database operations expanded in pkg/datastore/ with new table modules. All new code follows clean architecture with clear separation between CLI adapters (cmd/), application services (pkg/scanner, pkg/detector), domain logic (pkg/entities), and infrastructure (pkg/datastore, pkg/hash).

## Complexity Tracking

> **No complexity violations** - All constitution principles are met without exceptions.

---

## Planning Phases Completed

### ✅ Phase 0: Research & Analysis

**Output**: [research.md](research.md)

Research completed for:
1. **Hash Algorithm Selection**: Go stdlib crypto packages (SHA-256/512, SHA3-256), stream-based hashing with 64KB buffers, performance analysis shows SHA-256 at 300MB/s, SHA-512 at 500MB/s, SHA3-256 at 150MB/s on modern CPUs
2. **Database Schema Design**: Dedicated tables (files, folders, scan_state) with foreign keys to root_folders, SQLite with WAL mode, indexes on hash_value and size columns for duplicate queries
3. **Folder Traversal Pattern**: filepath.Walk with custom WalkFunc, permission error handling via file marking, platform-agnostic path handling
4. **Progress Reporting**: Time-based atomic counters with 10-second interval (configurable), separate goroutine for progress display, non-blocking updates
5. **Duplicate Detection Algorithm**: Size pre-filter → hash comparison, folder signature computation using sorted child hashes, 50% similarity threshold for partial matches
6. **Checkpoint Strategy**: Folder-level checkpoints in scan_state table, atomic folder completion updates, resume logic skips completed folders
7. **Configuration Management**: Viper with YAML, global hash_algorithm key, progress_interval_seconds key, default values in code
8. **Output Formatting**: Table formatter with tree structure (└──) and size/count summaries, JSON formatter with schema matching query contract
9. **Testing Strategy**: 70/20/10 test pyramid (70% unit, 20% integration, 10% e2e), table-driven tests for hash/scanner/detector, fixtures with known duplicates
10. **Performance Optimizations**: Hash caching, batch database inserts (1000 files/transaction), size-based pre-filtering (skip hash if size unique), parallel hashing (worker pool pattern)
11. **Cross-Platform Considerations**: filepath.Abs/Join/Clean for paths, os.PathSeparator handling, case-sensitivity detection

All NEEDS CLARIFICATION items resolved.

### ✅ Phase 1: Design & Contracts

**Outputs**: 
- [data-model.md](data-model.md) - Database schema and entity definitions
- [contracts/scan-commands.md](contracts/scan-commands.md) - CLI scan command specifications
- [contracts/query-commands.md](contracts/query-commands.md) - CLI query command specifications
- [quickstart.md](quickstart.md) - User guide and workflows

**Design artifacts**:
1. **Entity Model**: File, Folder, ScanState, DuplicateFileSet entities defined with Go struct definitions
2. **Database Schema**: 
   - `files` table (file_id, folder_id, root_folder_id, name, size, hash_value, hash_algorithm, error_status, mod_time, created_at)
   - `folders` table (folder_id, root_folder_id, parent_folder_id, path, total_files, total_size, scan_completed, created_at, updated_at)
   - `scan_state` table (scan_state_id, root_folder_id, scan_type, last_folder_path, status, started_at, updated_at)
   - Indexes: idx_files_hash, idx_files_size, idx_files_folder, idx_folders_parent, idx_folders_root, idx_scan_state_root
3. **CLI Contracts**:
   - `dupectl scan all <root-path>` - Complete folder + file scan with --resume, --verbose, --rescan flags
   - `dupectl scan folders <root-path>` - Structure mapping only (defer hashing)
   - `dupectl scan files <root-path>` - Hash calculation for registered folders
   - `dupectl get duplicates` - Query results with --json, --min-count, --root, --min-size, --sort flags
4. **User Workflows**: Quick start (5 minutes), large archive handling, incremental updates, interruption recovery, troubleshooting guide
5. **Agent Context Update**: GitHub Copilot instructions updated with Go 1.21+ and SQLite technologies

### ⏸ Phase 2: Task Decomposition

**Status**: Out of scope for `/speckit.plan` command.

**Next step**: User should execute `/speckit.tasks` command to generate tasks.md with:
- Task breakdown (database, scanner, hasher, detector, CLI, formatter, testing)
- Priority ordering and dependencies
- Acceptance criteria for each task
- Effort estimates

---

## Planning Summary

**Branch**: `001-duplicate-scan-system`  
**Plan Location**: `C:\Data\git\dupectl\specs\001-duplicate-scan-system\plan.md`

**Generated Artifacts**:
1. ✅ `plan.md` - This implementation plan
2. ✅ `research.md` - Technical research and decisions (Phase 0)
3. ✅ `data-model.md` - Database schema and entities (Phase 1)
4. ✅ `contracts/scan-commands.md` - Scan command specifications (Phase 1)
5. ✅ `contracts/query-commands.md` - Query command specifications (Phase 1)
6. ✅ `quickstart.md` - User guide (Phase 1)
7. ✅ `.github/agents/copilot-instructions.md` - Agent context updated

**Phase Status**:
- Phase 0 (Research): ✅ Complete
- Phase 1 (Design): ✅ Complete
- Phase 2 (Tasks): ⏸ Awaiting `/speckit.tasks` command

**Constitution Compliance**: All 12 principles verified ✅

**Ready for Implementation**: Yes - all design artifacts complete, no blockers identified.
